{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group: Wen-Hsuan Hung, Hongyuan Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEt7_cI9U9Tj"
   },
   "source": [
    "# EDGAR - Reading Information Tables in Text Format - Advanced Text Mining (65 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBwAmd47U9Ts"
   },
   "source": [
    "#### So far, we have collected CIKs for each of the Mutual Funds, then looked up the links of all the 13F HRs and the Information Tables, and identified them either text tables or xml tables. In class, we  obtained the information from the xml tables.  In this assignment we will obtain information from the text file.  These are not as nicely structured as the xml output.   In a csv (HW_Mutual_Fund_Info_Table_Link.csv) you will find a few of these links.  Your goal for this part of the homework is to obtain the link to the text files from the attached file  (HW_Mutual_Fund_Info_Table_Link.csv).  Then you will write a code that will go to the links of all the linked text files, and extract some columns.  Do not use Beutiful Soup for this assignment.   We provide some initial code to guide your initial steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjHXO52qU9Tu"
   },
   "source": [
    "#### First we have to collect the links of the text Info Tables in lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n6zV6DEyU9Tv"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'HW_Mutual_Fund_Info_Table_Link.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cb7c6cbbdb68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HW_Mutual_Fund_Info_Table_Link.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'HW_Mutual_Fund_Info_Table_Link.csv'"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "text_link = []\n",
    "text_Name = []\n",
    "text_date = []\n",
    "\n",
    "input_file = open('HW_Mutual_Fund_Info_Table_Link.csv', 'r')\n",
    "\n",
    "rows = input_file.readlines()\n",
    "input_file.close()\n",
    "\n",
    "# Your Code on Enumerating the Lists.  The result should be three lists, text_link, text_Name,\n",
    "#and text_date.  Each should have length 122.\n",
    "\n",
    "for i in range(1, len(rows)):\n",
    "    entrylist = rows[i].split(\",\")\n",
    "    if entrylist[4] == \"text\":\n",
    "        text_Name.append(entrylist[1])\n",
    "        text_date.append(entrylist[2])\n",
    "        text_link.append(entrylist[3])\n",
    "print(len(text_Name))\n",
    "print(len(text_date))\n",
    "print(len(text_link)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHJb_m4RU9Ty"
   },
   "source": [
    "#### Keep only the links that correspond to a date after 2010 (Don't inlude 2010, start at 2011).  Hint: you can use the datetime library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "o-XHtVwsU9T0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-efddd40fc5a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Enter code here to to keep only the dates corresponding to after 2010.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mentrylist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentrylist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rows' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#Use the following list to store the filtered values.\n",
    "filtered_dates = []\n",
    "filtered_name = []\n",
    "filtered_link = []\n",
    "\n",
    "#Enter code here to to keep only the dates corresponding to after 2010.  \n",
    "\n",
    "for i in range(1, len(rows)):\n",
    "    entrylist = rows[i].split(\",\")\n",
    "    x = datetime.strptime(entrylist[2], '%Y-%m-%d')\n",
    "    if entrylist[4] == \"text\" and x.year > 2010:\n",
    "        filtered_name.append(entrylist[1])\n",
    "        filtered_dates.append(entrylist[2])\n",
    "        filtered_link.append(entrylist[3])\n",
    "print(len(filtered_name))\n",
    "print(len(filtered_dates))\n",
    "print(len(filtered_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adirondack_Funds', 'ADVANCE_CAPITAL_I_INC', 'ADVANCE_CAPITAL_I_INC', 'ADVANCE_CAPITAL_I_INC', 'ADVANCE_CAPITAL_I_INC', 'ADVANCE_CAPITAL_I_INC', 'ADVANCE_CAPITAL_I_INC', 'Global_X_Funds']\n",
      "['2013-05-06', '2013-05-01', '2013-02-08', '2012-11-07', '2012-08-09', '2012-05-15', '2012-02-02', '2011-02-14']\n",
      "['/Archives/edgar/data/1311981/000116204413000513/0001162044-13-000513.txt', '/Archives/edgar/data/813470/000081347013000006/0000813470-13-000006.txt', '/Archives/edgar/data/813470/000081347013000001/0000813470-13-000001.txt', '/Archives/edgar/data/813470/000081347012000023/0000813470-12-000023.txt', '/Archives/edgar/data/813470/000081347012000019/0000813470-12-000019.txt', '/Archives/edgar/data/813470/000081347012000014/0000813470-12-000014.txt', '/Archives/edgar/data/813470/000081347012000003/0000813470-12-000003.txt', '/Archives/edgar/data/1432353/000114420411008428/0001144204-11-008428.txt']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_name)\n",
    "print(filtered_dates)\n",
    "print(filtered_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your filtered list should now have 8 elements.  These represent 3 mutual funds.   The first one has CIK=1311981 (from the link you can find this at data/1311981 ).  The second was has CIK 813470.  The third one has CIK 1432353. \n",
    "\n",
    "['https://www.sec.gov/Archives/edgar/data/1311981/000116204413000513/0001162044-13-000513.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347013000006/0000813470-13-000006.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347013000001/0000813470-13-000001.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347012000023/0000813470-12-000023.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347012000019/0000813470-12-000019.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347012000014/0000813470-12-000014.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/813470/000081347012000003/0000813470-12-000003.txt',\n",
    " 'https://www.sec.gov/Archives/edgar/data/1432353/000114420411008428/0001144204-11-008428.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd6_lJdpU9T1"
   },
   "source": [
    "#### Next, for each text link, extract the name of issuer, CUSIP, and the Quantity of shares.  You will also want to keep track of the mutual fund name as well as the filing report date.  \n",
    "\n",
    "#### Your output file should have 5 columns.  The first is the issue date of the form which can be found in the filtered_date list (this will be repeated for the same form).  The second is the mutual fund name which can be found in the filtered_name list (this will be repeated).  The third, fourth and fifith are the name of issuer, CUSIP, and shares respectively.  Make sure to account for the fact that while some of the text files have the same formatting, others do not.  This means you will have to look through them to make sure your code works for the each text file. (Please make sure you use one chunk of code to process all the URLs, i.e. if we were to change the list of URLs your code should still work for those new URLs. Do not process the URLs separately.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sXqi-EVEHxv"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SaMnUWdCT-g"
   },
   "source": [
    "#### Since we will be loading a page for multiple URLs, sometimes the website can block us off since we will be requesting the many sites from the same IP address. We can bypass this issue by using something called as a headless browser. In the headless browser, we create a list of browsers and their header information which includes user agents, and then for every time we load a certain URL and read the HTML content, we use the code to randomly choose a browser header and user agent, thus fooling the server into believing that every time a different browser is knocking them, and so the server will be less likely to block us!\n",
    "\n",
    "#### The following code has been provided to create this list of headers which you can randomly choose from later for each URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9Le_zH6VCJ2G"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import random \n",
    "from collections import OrderedDict\n",
    "\n",
    "headers_list = [\n",
    "    # Firefox 77 Mac\n",
    "    {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    },\n",
    "    # Firefox 77 Windows\n",
    "    {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    },\n",
    "    # Chrome 83 Mac\n",
    "    {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "    \"Sec-Fetch-Site\": \"none\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\"\n",
    "    },\n",
    "    # Chrome 83 Windows \n",
    "    {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-User\": \"?1\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjCf8WTSDXSt"
   },
   "source": [
    "Hint: If you wish to obtain the HTML content for a particular URL, you can use the following code:\n",
    "\n",
    "```\n",
    "    headers = random.choice(headers_list)\n",
    "    r = requests.Session()\n",
    "    r.headers = headers\n",
    "    html = r.get(url).text\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kpiR1WBENbK"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "Q6R4zHcVU9T2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1311981/000116204413000513/0001162044-13-000513.txt\n",
      "96\n",
      "96\n",
      "96\n",
      "96\n",
      "96\n",
      "https://www.sec.gov/Archives/edgar/data/813470/000081347013000006/0000813470-13-000006.txt\n",
      "437\n",
      "437\n",
      "437\n",
      "437\n",
      "437\n",
      "https://www.sec.gov/Archives/edgar/data/813470/000081347013000001/0000813470-13-000001.txt\n",
      "789\n",
      "789\n",
      "789\n",
      "789\n",
      "789\n",
      "https://www.sec.gov/Archives/edgar/data/813470/000081347012000023/0000813470-12-000023.txt\n",
      "1129\n",
      "1129\n",
      "1129\n",
      "1129\n",
      "1129\n",
      "https://www.sec.gov/Archives/edgar/data/813470/000081347012000019/0000813470-12-000019.txt\n",
      "1469\n",
      "1469\n",
      "1469\n",
      "1469\n",
      "1469\n",
      "https://www.sec.gov/Archives/edgar/data/813470/000081347012000014/0000813470-12-000014.txt\n",
      "1828\n",
      "1828\n",
      "1828\n",
      "1828\n",
      "1828\n",
      "https://www.sec.gov/Archives/edgar/data/813470/000081347012000003/0000813470-12-000003.txt\n",
      "2169\n",
      "2169\n",
      "2169\n",
      "2169\n",
      "2169\n",
      "https://www.sec.gov/Archives/edgar/data/1432353/000114420411008428/0001144204-11-008428.txt\n",
      "2268\n",
      "2268\n",
      "2268\n",
      "2268\n",
      "2268\n"
     ]
    }
   ],
   "source": [
    "issue_date = []\n",
    "mutual_fund_name = [] \n",
    "name_of_issuer = [] \n",
    "CUSIP = [] # CUSIP number\n",
    "shares = [] # No. of Shares of the company in the Mutual Fund\n",
    "\n",
    "# Your code goes here\n",
    "repeat = []\n",
    "number = 0\n",
    "\n",
    "for url in filtered_link:\n",
    "    link = \"https://www.sec.gov\" + url\n",
    "    print(link)  \n",
    "\n",
    "    headers = random.choice(headers_list)\n",
    "    r = requests.Session()\n",
    "    r.headers = headers\n",
    "\n",
    "    html = r.get(link).text\n",
    "\n",
    "    html = html.lower()\n",
    "\n",
    "    start = html.find('name of issuer')\n",
    "    end = html.find('</document>')\n",
    "    html2 = html[start:end]\n",
    "    html2 = html2.replace(\" adr \", \" adr  \")\n",
    "    html2 = html2.replace(\" $0.01 \", \" $0.01   \")\n",
    "    html2 = html2.replace(\" $.001 \", \" $0.01   \")\n",
    "    html2 = html2.replace(\" com com \", \"  com com \")\n",
    "    html2 = html2.replace(\" $ \", \"  \")\n",
    "    html2 = html2.replace(\" 892 \", \" 892  \")\n",
    "    html2 = html2.replace(\" sole \", \"  sole \")\n",
    "    html2 = html2.replace(\"<s>\", \" \")\n",
    "    html2 = html2.replace(\"<c>\", \" \")\n",
    "    html2 = html2.replace(\"-\", \" \")\n",
    "    html2 = html2.replace(\" ma \", \" \")\n",
    "    html2 = html2.replace(\" new \", \" \")\n",
    "    html2 = html2.replace(\" cl \", \" \")\n",
    "    html2 = html2.replace(\"cl b \", \" \")\n",
    "    html2 = html2.replace(\" rep \", \" \")\n",
    "    html2 = html2.replace(\" pfd \", \" \")\n",
    "    html2 = html2.replace(\" n v \", \" \")\n",
    "    html2 = html2.replace(\" sek \", \" \")\n",
    "    html2 = html2.replace(\" ser \", \" \")\n",
    "    html2 = html2.replace(\" pref \", \" \")\n",
    "\n",
    "    start = html2.find(\"none\\n\")+5\n",
    "    html2 = html2[start:]\n",
    "\n",
    "    #print(html2.splitlines())\n",
    "\n",
    "    html2 = html2.splitlines()\n",
    "\n",
    "    for i in range(0, len(html2)):\n",
    "        entrylist = html2[i].split(\"  \")\n",
    "        try:\n",
    "            while True:\n",
    "                entrylist.remove('')\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        if len(entrylist) > 4 and entrylist[0] != \"name of issuer\":\n",
    "            name_of_issuer.append(entrylist[0])\n",
    "            CUSIP.append(entrylist[2])\n",
    "            shares.append(entrylist[4])  \n",
    "            issue_date.append(filtered_dates[number])\n",
    "            mutual_fund_name.append(filtered_name[number])\n",
    "            \n",
    "    print(len(name_of_issuer))\n",
    "    print(len(CUSIP))\n",
    "    print(len(shares))\n",
    "    print(len(issue_date))\n",
    "    print(len(mutual_fund_name))\n",
    "    number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ct97shIvU9T2"
   },
   "source": [
    "#### Finally, you write down the lists to an output CSV or Text file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuWbMSrywBHg"
   },
   "source": [
    "**Don't forget to submit your output files to Canvas as well!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/Hongyuan Zhang/Desktop/MGTA414'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "BlWb9X1PU9T3"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "Output_File = open(\"Files_Directory/Edgar/A1PartA.txt\", \"w\")\n",
    "\n",
    "Output_File.write(\"name_of_issuer\\t\\t\\t\\tCUSIP\\t\\t\\tshares\\t\\t\\tissue_date\\t\\tmutual_fund_name\\n\")\n",
    "\n",
    "result = \"\"\n",
    "for x in range(0, len(name_of_issuer)):\n",
    "    result += name_of_issuer[x].ljust(40) + CUSIP[x].ljust(25) +  shares[x].ljust(25) + issue_date[x].ljust(25) + mutual_fund_name[x].ljust(25) + '\\t' + '\\n'\n",
    "\n",
    "Output_File.write(result)\n",
    "Output_File.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_1_Part_A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
